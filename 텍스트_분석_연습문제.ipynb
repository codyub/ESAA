{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codyub/ESAA/blob/main/%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%B6%84%EC%84%9D_%EC%97%B0%EC%8A%B5%EB%AC%B8%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **| 텍스트 분석 연습 문제**\n",
        "\n",
        "- 출처 : 캐글"
      ],
      "metadata": {
        "id": "Yw5mfB-1YfRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Tokenization**\n",
        "\n",
        "In the field of Natural Language Processing, tokenization basically refers to splitting up a larger body of text into smaller lines or words.\n",
        "\n",
        "There are mainly two types of tokenization :\n",
        "\n",
        "- Sentence Tokenization\n",
        "- Word Tokenization"
      ],
      "metadata": {
        "id": "zZBGXubsY6lE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import package\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize "
      ],
      "metadata": {
        "id": "zpux756aZRgB",
        "outputId": "54d3ea70-3eb7-4449-80d0-9a1136051d47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample text to perform our operations\n",
        "text = \"Hi, My name is Amartya Nambiar. I am a Computer Science Engineer. My favourite color is black\""
      ],
      "metadata": {
        "id": "-vKDqW1WZcjr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 토큰화\n",
        "sent_tokenize(text) "
      ],
      "metadata": {
        "id": "GowligokZeEA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48c7d924-5dc7-49d8-a59e-0898f0661949"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi, My name is Amartya Nambiar.',\n",
              " 'I am a Computer Science Engineer.',\n",
              " 'My favourite color is black']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 토큰화, 길이 출력\n",
        "words = word_tokenize(text)\n",
        "print(len(words))\n",
        "print(words)"
      ],
      "metadata": {
        "id": "pY1VFCkVaDrQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9ee6826-193a-4dd3-a23c-315d16f7f3b4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "['Hi', ',', 'My', 'name', 'is', 'Amartya', 'Nambiar', '.', 'I', 'am', 'a', 'Computer', 'Science', 'Engineer', '.', 'My', 'favourite', 'color', 'is', 'black']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Stopwords & Flushing them**\n",
        "\n",
        "Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence."
      ],
      "metadata": {
        "id": "unjlrUQTaiGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords') \n",
        "from nltk.corpus import stopwords  "
      ],
      "metadata": {
        "id": "Rf5p8-7KazcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "905ff9a6-4df2-4489-af96-beb488d08849"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# english stopword 불러오기, 15개만 확인\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "print(len(stop))\n",
        "print(stop[:15])"
      ],
      "metadata": {
        "id": "f8kqXiktbBSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4994293-14e0-4d3b-9e53-2f2c9d2eff02"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필터링을 통해 text에서 stopword 제거\n",
        "clean = [i for i in words if not i in stop]\n",
        "print(len(clean))\n",
        "print(clean)"
      ],
      "metadata": {
        "id": "CTeQujmRbPZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5ed8c95-5a18-446f-f9ac-4e569a7e54ad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "['Hi', ',', 'My', 'name', 'Amartya', 'Nambiar', '.', 'I', 'Computer', 'Science', 'Engineer', '.', 'My', 'favourite', 'color', 'black']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- is와 같은 것들이 사라졌음을 알 수 있다. 근데 소문자 대문자 I는 안 사라지네?\n",
        "- 여튼 4개가 사라졌다."
      ],
      "metadata": {
        "id": "DRy5cUhUEqYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 소문자도 사라지게 하기\n",
        "words =word_tokenize(text.lower()) # 소문자로 모두 바꿔버리기\n",
        "clean_lower = [i for i in words if not i in stop]\n",
        "print(len(clean_lower))\n",
        "print(clean_lower)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsVMmUtlFAzP",
        "outputId": "8075635e-5cd3-4b96-97bd-1ad1edcac563"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n",
            "['hi', ',', 'name', 'amartya', 'nambiar', '.', 'computer', 'science', 'engineer', '.', 'favourite', 'color', 'black']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 이제 3개가 더 사라졌다. 대문자 I도 사라짐"
      ],
      "metadata": {
        "id": "NzvE0M23FG1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# punctuation('.', ',') 제거\n",
        "import string \n",
        "punctuations = list(string.punctuation)\n",
        "stop += punctuations # stop word에 불필요한 점들 추가\n",
        "words = word_tokenize(text.lower()) # 소문자로 만든 단어들 다시 돌리기\n",
        "clean_lower = [i for i in words if not i in stop]\n",
        "print(len(clean_lower))\n",
        "print(clean_lower)"
      ],
      "metadata": {
        "id": "lxAH2ytMb3TY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b076386-ede0-4618-ee5b-6850e8e095f9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "['hi', 'name', 'amartya', 'nambiar', 'computer', 'science', 'engineer', 'favourite', 'color', 'black']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 총 6개의 단어와 점 등이 사라졌다."
      ],
      "metadata": {
        "id": "A51qf7XiFT6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Stemming**\n",
        "\n",
        "Stemming is a technique used to extract the base form of the words by removing affixes from them. It is just like cutting down the branches of a tree to its stems. For example, the stem of the words eating, eats, eaten is eat.\n",
        "\n",
        "There are mainly two widely used Stemmer Algorithms:\n",
        "\n",
        "- Porter Stemmer (we'll work on this)\n",
        "- Lancaster Stem"
      ],
      "metadata": {
        "id": "yEAlzfMhcBcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "EQQuNe0McNBg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ps 객체 생성 후 stemming , example 최소 3개 임의 생성 후 시도해보기\n",
        "# example1= ['helps', 'helping', 'helped']\n",
        "ps = PorterStemmer() \n",
        "example = ['helps','helping','helped']   \n",
        "stemmed_example = [ps.stem(i) for i in example]\n",
        "stemmed_example"
      ],
      "metadata": {
        "id": "hp_atqYwdkR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f2a95a-bf2b-4157-e6cf-0c83afc3fa54"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['help', 'help', 'help']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps.stem('happiness') # but it isn't always the best choice"
      ],
      "metadata": {
        "id": "mwMDJ3ZaduyK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8cb8936c-7b0d-4735-9a25-cbca86c9dcb0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'happi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 단어의 형태가 활용되면서 변화하면 못 알아 듣는 경우가 있다는 것을 알 수 있다. "
      ],
      "metadata": {
        "id": "6Kiyt2zDFzx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Parts of Speech**\n",
        "\n",
        "To know what is the context of a particular word\n",
        "\n",
        "For example : Shyam is a Proper Noun, Desk is a Noun and Happy is an adjective.\n",
        "\n",
        "-pos_tag가 문장내 단어의 품사에 대한 정보를 반영해줌. 함께 들어 있는 것은 pos_tag에서 정한 약자로, 각각의 품사 의미가 있으므로, 알고싶으면 참고하면 됨\n",
        "- ex. JJ: adjective, NN:noun, singular"
      ],
      "metadata": {
        "id": "71eO1YE1dwBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.corpus import movie_reviews\n",
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "text = movie_reviews.raw(\"neg/cv954_19932.txt\") "
      ],
      "metadata": {
        "id": "S2iVOPi4eEKC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02fba3ee-248e-437a-e92a-97284fdacfee"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 스피치 데이터를 다운로드 받음"
      ],
      "metadata": {
        "id": "90yA23FKI_60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "eFqP4aTdIURz",
        "outputId": "f7230de6-0bbc-4b2f-e921-d3c2a4ed3ce5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a new entry in the \" revisionist history \" genre of filmmaking , dick suggests that two not-too-bright teenage girls are the cause of the uncovering of the nation\\'s biggest presidential scandal . \\nkirsten dunst and michelle williams star betsy and arlene , who while trying to deliver a fan letter from arlene\\'s watergate hotel room , accidentally stumble across g . gordon liddy ( played dead-on by harry shearer ) and the infamous break-in . \\nwhen they recognize liddy later on during a white house field trip , they are ushered into a conference room , questioned as to what they know , and leave as official presidential dog walkers . \\nthe girls manage to unwittingly uncover every bit of the watergate scandal while performing their duties , but have no clue as to what they are getting involved with . \\nwhen they discover that nixon ( another dead-on performance by dan hedaya , who actually favors nixon slightly , unlike anthony hopkins ) has been abusive to checkers , the presidential dog , thanks to the conversations that he always recorded , they quit and become disillusioned . \\nduring a prank phone call the girls make to woodward and bernstein , events are set into motion that eventually lead to the president\\'s resignation . \\nthis film starts off promisingly with an aged woodward and bernstein arguing with each other on an obvious larry king-type talk show ( featuring a cameo by french stewart ) about revealing the identity of \" deep throat \" . \\nfrom there , we are subjected to bodily function humor and just about every bad \" dick \" joke one can derive from this type of supposed comedy . \\nat one point , the girls are having to scream over a high school band playing on the steps of the lincoln memorial . \\nthe band manages to stop right as dunst screams \" you have to stop letting dick run your life ! \" \\nmuch to the horror of everyone standing within earshot . \\nseveral other variations on this wordplay surface all throughout the film . \\nif this movie had been smarter i would have been less likely to fault it\\'s juvenile bathroom humor , but it\\'s not . \\nthe film was apparently made for relatively younger people because every major player in the watergate scandal is introduced and shoved down the audience\\'s throat in the least subtle way possible . \\ni don\\'t recall oliver stone\\'s nixon having to pander to it\\'s audience , but of course that film wasn\\'t a comedy aimed squarely at a 13-20 year-old film going audience . \\nthe only redeeming thing about this movie is it\\'s remarkable supporting cast . \\ni wanted to see more of ferrell and mcculloch\\'s woodward and bernstein . \\nthose two characters are the sole basis for my rating . \\ni wish they had been given more screen time , but unfortunately , they are only relegated to the final half-hour . \\ntheir constant bickering and fighting over trying to get the story are a major highlight , especially mcculloch\\'s constant thwarting of ferrell\\'s attempts to gather information from the girls ( who , in the course of the narrative are revealed as deep throat , so named thanks to an ill planned trip to a porno theater by betsy\\'s brother ) . \\nthe other members of the cast are excellent in their portrayals of their particular characters , but are given nothing to work with . \\ni\\'d like to see the same cast portray these characters in a script more suited towards their comedic abilities . \\nas for the two leads , dunst and williams can definitely do better . \\nthey come off as what could best be described as romy and michele : the early years in this particular film , a highly dubious distinction at best . \\nstay through the first half of the end credits though , to see an interesting scene involving dunst and williams suggestively sucking on lollipops emblazoned with the title of the movie . \\nan excellent idea marred by poor execution , dick could have been a great movie . \\nless of the juvenile humor and more of the smarter comedy displayed by the woodward and bernstein scenes , could have made this film a wonderful satire of the nixon presidency as seen through the eyes of two naive fifteen year olds . \\nas it stands though , dick offers nothing but what filmmaker kevin smith so accurately defines as \" dick and poopie \" jokes . \\nand that , to me , does not make a funny movie . \\n[pg-13] \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos = pos_tag(word_tokenize(text))     #applying pos_tag()\n",
        "pos[1:5] "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSrQYorbIaE8",
        "outputId": "17bed9a9-6e94-4a7d-99b7-825d5dbdba81"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('new', 'JJ'), ('entry', 'NN'), ('in', 'IN'), ('the', 'DT')]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Lemmatization(표제어 추출)**\n",
        "\n",
        "PorterStemmer class chops off the suffixes from the word but this isn't the best thing to apply to clean our data.\n",
        "\n",
        "Stemming technique only looks at the form of the word whereas Lemmatization technique looks at the meaning of the word. It means after applying lemmatization, we will always get a valid word.\n",
        "\n",
        "- 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단\n",
        "- 예를 들어서 am, are, is는 서로 다른 스펠링이지만 그 뿌리 단어는 be라고 볼 수 있습니다. 이때, 이 단어들의 표제어는 be"
      ],
      "metadata": {
        "id": "NrFml-IJepQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import package\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lem = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "xWF0Ibznetk6"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XM3KBjUzIiLT",
        "outputId": "b44561d9-d490-45bd-b85b-18af8e62786f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatize 'believes', 'happiness'\n",
        "lem.lemmatize('believes')"
      ],
      "metadata": {
        "id": "C-abWCqffiwP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6bfc035b-3f3d-46c7-c591-814ef6834769"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'belief'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lem.lemmatize('happiness') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FTExIZarImRx",
        "outputId": "5c9a26e6-4f7c-4249-d776-12a25ffbc808"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'happiness'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lem.lemmatize('happier',pos = 'a') #  a:adjective, n:noun 등 pos 옵션 가능"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "guRsYkehGdyk",
        "outputId": "aa1ad6d9-6ad5-4959-ee07-9e2ea0b3369e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'happy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}